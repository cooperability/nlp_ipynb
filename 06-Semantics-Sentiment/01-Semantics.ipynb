{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6888b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Need to download the larger Spacy models in order to use its vector embeddings\n",
    "python3 -m spacy download en_core_web_lg\n",
    "Word2vec is a two-layer neural net for text processing\n",
    "its input is a corpus and its output is a set of feature vectors for words in that corpus\n",
    "-purpose-built to group the vectors of similar words together in vectorspace\n",
    "-mathematically detects cosine similarities between words represented by a 300-dimension vector\n",
    "-once a w2v model is trained, it can make accurate guesses about a word's in-context meaning\n",
    "1. use context to predict a target word (CBOW continuous bag of words)\n",
    "2. use a word to predict a target context (skip-gram)\n",
    "use vector arithmetic such as `new_v = king-man+woman` or `new_v closest to vector for queen`\n",
    "\"\"\"\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ce136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(514157, 300)\n"
     ]
    }
   ],
   "source": [
    "#Same number of vector dimensions for documents and words =300\n",
    "print (nlp(u'lion').vector.shape)\n",
    "print(nlp.vocab.vectors.shape) #hundreds of thousands of words, each represented by 300D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a4cac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First group:\n",
      "lion lion 1.0\n",
      "lion cat 0.3854507803916931\n",
      "lion pet 0.20031584799289703\n",
      "cat lion 0.3854507803916931\n",
      "cat cat 1.0\n",
      "cat pet 0.732966423034668\n",
      "pet lion 0.20031584799289703\n",
      "pet cat 0.732966423034668\n",
      "pet pet 1.0\n",
      "\n",
      " Second group:\n",
      "like True 50.609623 False\n",
      "like 1.0\n",
      "love 0.5212638974189758\n",
      "hate 0.5065141320228577\n",
      "blarg 0.0\n",
      "love True 58.563564 False\n",
      "like 0.5212638974189758\n",
      "love 1.0\n",
      "hate 0.5708349943161011\n",
      "blarg 0.0\n",
      "hate True 46.569798 False\n",
      "like 0.5065141320228577\n",
      "love 0.5708349943161011\n",
      "hate 1.0\n",
      "blarg 0.0\n",
      "blarg False 0.0 True\n",
      "like 0.0\n",
      "love 0.0\n",
      "hate 0.0\n",
      "blarg 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperreed/anaconda3/envs/nlp_ipynb/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n"
     ]
    }
   ],
   "source": [
    "#related words' score is calculated as the square root of the sum of squared vectors\n",
    "print(\"First group:\")\n",
    "tokens = nlp(u'lion cat pet')\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text,token2.text,token1.similarity(token2))\n",
    "\n",
    "\n",
    "print(\"\\n Second group:\")\n",
    "tokens2 = nlp(u\"like love hate blarg\")\n",
    "for token1 in tokens2:\n",
    "    print(token1.text,\n",
    "        token1.has_vector, #is it in the vocabulary?\n",
    "        token1.vector_norm, #What's the normalized vector\n",
    "        token1.is_oov), #out of vocabulary\n",
    "    for token2 in tokens2:\n",
    "        print(token2.text,\n",
    "              token1.similarity(token2))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edcbeeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['government', 'that', 'and', 'those', 'should', 'king', 'they', 'would', 'cause', 'these']\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "cosine_similarity = lambda vec1,vec2: 1-spatial.distance.cosine(vec1,vec2)\n",
    "king = nlp.vocab['government'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "new_v = king\n",
    "computed_similarities = []\n",
    "for word in nlp.vocab:    #for 500K+ words in vocab\n",
    "    w_v = word.vector\n",
    "    if word.has_vector and word.is_lower and word.is_alpha:\n",
    "        similarity = cosine_similarity(new_v,w_v)\n",
    "        computed_similarities.append((word,similarity))\n",
    "computed_similarities = sorted(computed_similarities,key=lambda item:-item[1])\n",
    "#without the minus in the line above, they would be sorted in descending order\n",
    "#i.e. starting with the LEAST similar words. Here we sort for the MOST similar.\n",
    "print([t[0].text for t in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3e141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10645a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
