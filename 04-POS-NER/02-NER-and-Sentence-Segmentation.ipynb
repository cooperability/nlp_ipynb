{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80831d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc:\n",
      "No entities in document\n",
      "\n",
      " doc1:\n",
      "May // DATE // Absolute or relative dates or periods\n",
      "1 2 3 6\n",
      "500 dollars // MONEY // Monetary values, including unit\n",
      "4 6 14 25\n",
      "The Mona Lisa // WORK_OF_ART // Titles of books, songs, etc.\n",
      "8 11 33 46\n",
      "Louvre // LOC // Non-GPE locations, mountain ranges, bodies of water\n",
      "13 14 54 60\n",
      "Paris // GPE // Countries, cities, states\n",
      "15 16 64 69\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Named Entity Recognition (NER) is the process of locating & classifying named entity mentions\n",
    "in unstructured text. Categories usually include person names, organizations, locations,\n",
    "medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\"\"\"\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text + ' // ' \n",
    "                  + ent.label_ + ' // ' \n",
    "                  + str(spacy.explain(ent.label_)))\n",
    "            print(ent.start, ent.end,     #the token span's \"start\" & \"end\" index position in doc\n",
    "                 ent.start_char, ent.end_char) #The entity text's \"start\" and \"end\" index in doc\n",
    "    else:\n",
    "        print('No entities in document')\n",
    "doc=nlp(u'Hello where are we?')\n",
    "print(\"doc:\")\n",
    "show_ents(doc) #no entities in doc\n",
    "doc1=nlp(u\"In May I paid 500 dollars to see The Mona Lisa at the Louvre in Paris.\")\n",
    "print(\"\\n doc1:\")\n",
    "show_ents(doc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefa7b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before manually recognizing:\n",
      "300 // MONEY // Monetary values, including unit\n",
      "4 5 18 21\n",
      "\n",
      " After manually recognizing:\n",
      "Grubbo // ORG // Companies, agencies, institutions, etc.\n",
      "0 1 0 6\n",
      "300 // MONEY // Monetary values, including unit\n",
      "4 5 18 21\n"
     ]
    }
   ],
   "source": [
    "#Create an unknown entity and you'll need to manually set its recognition\n",
    "doc2=nlp(u\"Grubbo will sell $300m in stock.\")\n",
    "print(\"Before manually recognizing:\")\n",
    "show_ents(doc2)\n",
    "from spacy.tokens import Span\n",
    "ORG=doc.vocab.strings[u\"ORG\"] #this returns 381 if called, the hash of ORG\n",
    "new_ent=Span(doc2,0,1,label=ORG) #from startindex up to but not including endindex\n",
    "doc2.ents=list(doc2.ents) + [new_ent]\n",
    "print(\"\\n After manually recognizing:\")\n",
    "show_ents(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdf5878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before custom PhraseMatcher:\n",
      "No entities in document\n",
      "\n",
      " after custom PhraseMatcher:\n",
      "[(2689272359382549672, 4, 6), (2689272359382549672, 10, 13)]\n",
      "\n",
      " after new named entities added to show_ents function:\n",
      "vacuum cleaner // PRODUCT // Objects, vehicles, foods, etc. (not services)\n",
      "4 6 12 26\n",
      "vacuum-cleaner // PRODUCT // Objects, vehicles, foods, etc. (not services)\n",
      "10 13 37 51\n"
     ]
    }
   ],
   "source": [
    "#Add a new named entity for non-proper nouns that you're searching for in the text\n",
    "from spacy.matcher import PhraseMatcher\n",
    "doc3=nlp(u\"I got a new vacuum cleaner.\" u\"I love my vacuum-cleaner.\")\n",
    "\n",
    "print(\"before custom PhraseMatcher:\")\n",
    "show_ents(doc3)\n",
    "\n",
    "print(\"\\n after custom PhraseMatcher:\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['vacuum cleaner', 'vacuum-cleaner']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add('newproduct', None, *phrase_patterns)\n",
    "matches = matcher(doc3)\n",
    "print(matches)\n",
    "\n",
    "print(\"\\n after new named entities added to show_ents function:\")\n",
    "from spacy.tokens import Span\n",
    "PROD = doc.vocab.strings[u\"PRODUCT\"]\n",
    "new_ents = [Span(doc3,match[1],match[2],label=PROD) for match in matches]\n",
    "doc3.ents = list(doc3.ents) + new_ents\n",
    "show_ents(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06453a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How to catch multiple different named entities of the same type\n",
    "doc4 = nlp(u\"I bought for $500 and sold for 600 dollars\")\n",
    "len([ent for ent in doc4.ents if ent.label_ == \"MONEY\"])   #2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7fa5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before options specified:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    May\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " I paid \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    500 dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " to see \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Mona Lisa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " at the \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Louvre\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Paris\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sony\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    almost 10,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Walkman Music Players\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " after options specified:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In May I paid \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg,green,yellow); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    500 dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " to see \n",
       "<mark class=\"entity\" style=\"background: radial-gradient(orange,purple); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Mona Lisa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " at the Louvre in Paris.\n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sony\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold almost 10,000 \n",
       "<mark class=\"entity\" style=\"background: radial-gradient(orange,purple); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Walkman Music Players\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "print(\"before options specified:\")\n",
    "doc5=nlp(u\"In May I paid 500 dollars to see The Mona Lisa at the Louvre in Paris.\"\n",
    "        u\"Sony sold almost 10,000 Walkman Music Players.\")\n",
    "for sent in doc5.sents: #better for printing each sentence on a new line\n",
    "    displacy.render(nlp(sent.text),style='ent',jupyter=True)\n",
    "print(\"\\n after options specified:\")\n",
    "colors={'ORG':'green',\n",
    "        'WORK_OF_ART':'radial-gradient(orange,purple)',\n",
    "        'MONEY':'linear-gradient(90deg,green,yellow)'}\n",
    "options = {'ents':['WORK_OF_ART','ORG','MONEY'],'colors':colors}\n",
    "displacy.render(doc5,style='ent',jupyter=True,options=options)\n",
    "#If you'd like to serve outside of Jupyter, uncomment this line\n",
    "#displacy.serve(doc5,style='ent',options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3465aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single word return:\n",
      "This\n",
      "\n",
      "each sentence return:\n",
      "This is a sentence.\n",
      "This is also a sentence.\n",
      "Surprisingly, this too is a sentence.\n",
      "\n",
      " Single sentence return:\n",
      "This is a sentence.\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc6=nlp(u'This is a sentence. This is also a sentence. Surprisingly, this too is a sentence.')\n",
    "print(\"Single word return:\")\n",
    "print(doc6[0]) # very easy to grab tokens by index from the doc itself, but not from doc.sents\n",
    "print(\"\\neach sentence return:\")\n",
    "for sent in doc6.sents:\n",
    "    print(sent)\n",
    "\n",
    "#this is the best way to return a sentence object, but it returns Span not String\n",
    "print(\"\\n Single sentence return:\")\n",
    "print(list(doc6.sents)[0])\n",
    "print(type(list(doc6.sents)[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d0dba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. default segmentation rules:\n",
      " \"Management is bad; leadership is bad.\"\n",
      "\n",
      "\n",
      "-Gandhi\n",
      "\n",
      "\n",
      "\n",
      " 2. After adding segmentation rules:\n",
      "new pipeline:\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "new output:\n",
      " \"Management is bad; leadership is bad.\"\n",
      "-Gandhi\n",
      "\n",
      " 3. changing segmentation rules:\n",
      "Source string:\n",
      "This is a sentence. This is another. \n",
      "\n",
      " This is a \n",
      "third.\n",
      "Separated by sentence default:\n",
      "This is a sentence.\n",
      "This is another. \n",
      "\n",
      " \n",
      "This is a \n",
      "third.\n"
     ]
    }
   ],
   "source": [
    "#add or change SENTENCE SEGMENTATION rules to better split text\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc7 = nlp(u' \"Management is bad; leadership is bad.\" -Gandhi')\n",
    "\n",
    "print(\"1. default segmentation rules:\")\n",
    "for sent in doc7.sents:\n",
    "    print(sent)\n",
    "    print('\\n')\n",
    "    \n",
    "print(\"\\n 2. After adding segmentation rules:\")\n",
    "#we create a new component for our pipeline with additional sentence segmentation rules\n",
    "#need to register the custom component with a name before adding to pipeline\n",
    "@spacy.Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:  #we iterate from the start to the end -1...\n",
    "        if token.text == ';':\n",
    "            doc[token.i+1].is_sent_start = True  #to prevent a segmentation fault here\n",
    "    return doc\n",
    "\n",
    "# nlp.add_pipe(\"set_custom_boundaries\",before=\"parser\")\n",
    "print(\"new pipeline:\")\n",
    "print(nlp.pipe_names)# print(set_custom_boundaries(doc7))\n",
    "print(\"new output:\")\n",
    "for sent in doc7.sents:\n",
    "    print(sent)\n",
    "\n",
    "print(\"\\n 3. changing segmentation rules:\")\n",
    "mystring=u\"This is a sentence. This is another. \\n\\n This is a \\nthird.\"\n",
    "print(\"Source string:\")\n",
    "print(mystring)\n",
    "doc8=nlp(mystring)\n",
    "print(\"Separated by sentence default:\")\n",
    "for sentence in doc8.sents:\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c92750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4. Separated by custom SentenceSegmenter\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy' has no attribute 'SentenceSegmenter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q_/lbt0ys9j66d18h8b4fc_pcrw0000gn/T/ipykernel_91959/3724002678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msegmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentenceSegmenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_on_newline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new pipeline:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'spacy' has no attribute 'SentenceSegmenter'"
     ]
    }
   ],
   "source": [
    "#This code is outdated. Currently working to update this with 2023 spacy.\n",
    "print(\"\\n 4. Separated by custom SentenceSegmenter\")\n",
    "from spacy.pipeline import SentenceSegmenter\n",
    "def split_on_newline(doc):\n",
    "    start = 0\n",
    "    seen_newline=False\n",
    "    for word in doc:\n",
    "        if seen_newline:\n",
    "            yield doc[start:word.i]\n",
    "            start = word.i\n",
    "            seen_newline=False\n",
    "        elif word.text.startswith('\\n'):\n",
    "            seen_newline=True\n",
    "    yield doc[start:]\n",
    "\n",
    "segmented = SentenceSegmenter(nlp.vocab,strategy=split_on_newline)\n",
    "nlp.add_pipe(segmented)\n",
    "print(\"new pipeline:\")\n",
    "print(nlp.pipe_names)# print(set_custom_boundaries(doc7))\n",
    "print(\"new output:\")\n",
    "doc9=nlp(mystring)\n",
    "for sent in doc9.sents:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd889e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
