{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "074fd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stemming: the process whereby words are etymologically reduced to a root.\n",
    "spaCy intentionally doesn't include a stemmer, instead relying entirely on lemmatization,\n",
    "so we use nltk for stemming.\n",
    "\"\"\"\n",
    "import nltk\n",
    "words=['swam','swimmer','swims','swim',\n",
    "       'easily','easy','quickly','fairly', 'fairness',\n",
    "       'caresses','cares','ponies',\n",
    "       'relational','national','unconventional',\n",
    "       'generous','generously','generate','generosity','general','generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d16a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swam---->swam\n",
      "swimmer---->swimmer\n",
      "swims---->swim\n",
      "swim---->swim\n",
      "easily---->easili\n",
      "easy---->easi\n",
      "quickly---->quickli\n",
      "fairly---->fairli\n",
      "fairness---->fair\n",
      "caresses---->caress\n",
      "cares---->care\n",
      "ponies---->poni\n",
      "relational---->relat\n",
      "national---->nation\n",
      "unconventional---->unconvent\n",
      "generous---->gener\n",
      "generously---->gener\n",
      "generate---->gener\n",
      "generosity---->generos\n",
      "general---->gener\n",
      "generation---->gener\n"
     ]
    }
   ],
   "source": [
    "#Porter's Algorithm (1980): common and effective stemming tool which uses 5-phase word reduction.\n",
    "#1: suffix shortening, e.g. caresses->caress NOT cares, relational->relate NOT national->nate\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_s=PorterStemmer()\n",
    "for word in words:\n",
    "    print(word + '---->' + p_s.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49797642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swam----->swam\n",
      "swimmer----->swimmer\n",
      "swims----->swim\n",
      "swim----->swim\n",
      "easily----->easili\n",
      "easy----->easi\n",
      "quickly----->quick\n",
      "fairly----->fair\n",
      "fairness----->fair\n",
      "caresses----->caress\n",
      "cares----->care\n",
      "ponies----->poni\n",
      "relational----->relat\n",
      "national----->nation\n",
      "unconventional----->unconvent\n",
      "generous----->generous\n",
      "generously----->generous\n",
      "generate----->generat\n",
      "generosity----->generos\n",
      "general----->general\n",
      "generation----->generat\n"
     ]
    }
   ],
   "source": [
    "#Snowball is a stemming language also developed by Martin Porter\n",
    "#\"English Stemmer\" or \"Porter 2 Stemmer\" is below:\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_s=SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "    print(word + '----->' + s_s.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f84fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lemmatization: The process whereby words are broken into their etymological components.\n",
    "In contrast to stemming, lemmatization looks at a language's full morphology in order \n",
    "to go beyond word reduction and achieve meaning reduction.\n",
    "Spacy drops stemming in favor of lemmatization because lemmatization can yield much more \n",
    "informative semantic insights. Lemmatization looks at surrounding text to infer a word's\n",
    "meaning, not just the word itself. \n",
    "was to be, mice to mouse, meeting to meet or maybe meeting\n",
    "\"\"\"\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "902774b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON  -PRON-    561228191312463089\n",
      "'m          VERB  be        10382539506755952630\n",
      "a           DET   a         11901859001352538922\n",
      "swimmer     NOUN  swimmer   8984364056738817612\n",
      "swimming    VERB  swim      13054409096476681252\n",
      "swimmingly  ADV   swimmingly14521368103460307620\n",
      ";           PUNCT ;         631425121691394544\n",
      "at          ADP   at        11667289587015813222\n",
      "a           DET   a         11901859001352538922\n",
      "swim        NOUN  swim      13054409096476681252\n",
      "meet        NOUN  meet      6880656908171229526\n",
      ",           PUNCT ,         2593208677638477497\n",
      "I           PRON  -PRON-    561228191312463089\n",
      "swim        VERB  swim      13054409096476681252\n",
      "because     ADP   because   16950148841647037698\n",
      "I           PRON  -PRON-    561228191312463089\n",
      "swam        VERB  swam      10694386587443064459\n",
      ".           PUNCT .         12646065887601541794\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(u\"I'm a swimmer swimming swimmingly; at a swim meet, I swim because I swam.\")\n",
    "def show_lemmas(text):\n",
    "    for token in text: #lemma returns a lemma's hashcode; lemma_ returns the lemma itself\n",
    "        print(f'{token.text:{12}}{token.pos_:{6}}{token.lemma_:{10}}{token.lemma:{10}}') \n",
    "show_lemmas(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bee8b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n",
      "True\n",
      "False\n",
      "306\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Stop-words: ultra-common, semantically unhelpful, and sometimes harmful to nlp results.\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "print(nlp.vocab['is'].is_stop)\n",
    "print(nlp.vocab['btw'].is_stop) \n",
    "nlp.Defaults.stop_words.add('btw') #add a new word to vocab\n",
    "nlp.vocab['btw'].is_stop=True #add a word from vocab into stop_words; reverse to remove one\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "print(nlp.vocab['btw'].is_stop)\n",
    "nlp.vocab['btw'].is_stop=False #reset for next exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "718b14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 4 5 SolarPower\n",
      "8656102463236116519 SolarPower 9 12 Solar-power\n",
      "8656102463236116519 SolarPower 15 17 Solar power\n",
      "\n",
      "\n",
      " after punctuation rules: [(8656102463236116519, 0, 3), (8656102463236116519, 4, 5)]\n"
     ]
    }
   ],
   "source": [
    "#spacy has a rule-matching tool called matcher, similar to regex but more powerful\n",
    "from spacy.matcher import Matcher\n",
    "m=Matcher(nlp.vocab)\n",
    "pattern1=[{'LOWER':'solarpower'}] #detect SolarPower,  \n",
    "pattern2=[{'LOWER':'solar'},{'IS_PUNCT':True},{'LOWER':'power'}] #Solar-power,\n",
    "pattern3=[{'LOWER':'solar'},{'LOWER':'power'}] #Solar power\n",
    "m.add('SolarPower',None,pattern1,pattern2,pattern3)\n",
    "doc=nlp(u\"I'm so into SolarPower, I myself am Solar-power, I love Solar power. solar-----power.\")\n",
    "found=m(doc) #this will load the matches' hashes and indices into a data structure\n",
    "for match_id, start, end in found:\n",
    "    str_id=nlp.vocab.strings[match_id] #get string representation\n",
    "    span = doc[start:end]   #get the matched span\n",
    "    print(match_id, str_id, start, end, span.text)\n",
    "m.remove('SolarPower') #remove the prior set of patterns added\n",
    "\n",
    "\"\"\"\n",
    "now let's make a regex. Quantifiers for the 'OP' key: \n",
    "\\! requires zero matches\n",
    "? requires zero or one matches\n",
    "\\+ requires one or more matches\n",
    "\\* allows zero or more matches\n",
    "\"\"\"\n",
    "pattern1=[{'LOWER':'solarpower'}] #detect SolarPower,  \n",
    "pattern2=[{'LOWER':'solar'},{'IS_PUNCT':True,'OP':'*'},{'LOWER':'power'}] #Solar---------power\n",
    "doc2=nlp(\"Solar---power is solarpower.\")\n",
    "m.add('SolarPower', None, pattern1, pattern2)\n",
    "found=m(doc2)\n",
    "print(\"\\n\\n after punctuation rules:\",found)\n",
    "m.remove('SolarPower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d02ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[voodoo economics, supply-side economics, trickle-down economics, free-market economics]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q_/lbt0ys9j66d18h8b4fc_pcrw0000gn/T/ipykernel_6863/2821375657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#pass each document object individually into the matcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EconMatcher'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#keyword arguments or asterisks arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmatcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.Matcher.add\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmatcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher._convert_strings\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "with open('../TextFiles/reaganomics.txt',mode='rb') as f:\n",
    "    byte_string=f.read()\n",
    "    #replace any error characters in source bytes\n",
    "    text = byte_string.decode('utf8', errors='replace') \n",
    "    doc3=nlp(text)\n",
    "phrase_list=['voodoo economics','supply-side economics','trickle-down economics','free-market economics']\n",
    "#create a document object for each individual phrase\n",
    "patterns=[nlp(phrase) for phrase in phrase_list]\n",
    "print(patterns)\n",
    "pattern1=[{'LOWER':'voodoo economics'}]\n",
    "pattern2=[{'LOWER':'supply-side economics'}]\n",
    "pattern3=[{'LOWER':'trickle-down economics'}]\n",
    "\n",
    "\n",
    "#pass each document object individually into the matcher\n",
    "m2=Matcher(nlp.vocab)\n",
    "m2.add('EconMatcher',*patterns) #keyword arguments or asterisks arg\n",
    "found=m2(doc3)\n",
    "print(found)\n",
    "for match_id, start, end in found:\n",
    "    str_id=nlp.vocab.strings[match_id] #get string representation\n",
    "    span = doc3[start-5:end+5]   #get the matched span, plus five words on either side\n",
    "    print(match_id, str_id, start, end, span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c8f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
